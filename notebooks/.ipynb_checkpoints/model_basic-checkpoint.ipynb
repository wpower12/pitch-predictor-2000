{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Pitch Model\n",
    "This model provides a baseline to compare against.  The other models will attempt to show that they learn more useful patterns than just the pitch sequences themselves.  This will only take the pitches as input, with no additional features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    " The data for all the models will be the pitch sequences from all games in the MLB during the past 4 years (2016,2015,2014,2013). Two scripts were used to collect the data.  The first updates the lahmanDB of baseballs statistics to include the mlbgameID that is used by the python library that exposes the pitch sequence data. Once the database is updated to include the ids for the years to be sequenced, the second script is run.  This script reads the at bat data for each game in a given year and produces a set of example feature vectors.  These are saved in pickled format for later use.  \n",
    "\n",
    "For this model, just the sequences are needed. So the pickled files need to be read in, and converted to a format that only includes these sequences, and that can be dealt with by tensorflow.  Additionally, the sequences should be converted into one hot vectors. TODO-Find out a way to do this with tensorflow.  \n",
    "\n",
    "Once the entire data set has been formatted correctly, the data needs to be seperated into training, validation, and testing sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest sequence length: 18\n",
      "empties: 13757\n",
      "total (clean): 110216\n",
      "pitch types: 16\n",
      "SL 70932\n",
      "KC 9628\n",
      "CH 48577\n",
      "SI 34286\n",
      "FO 188\n",
      "FS 7154\n",
      "CU 40774\n",
      "KN 1504\n",
      "FC 23526\n",
      "FF 164046\n",
      "EP 14\n",
      "IN 1853\n",
      "SC 26\n",
      "FT 62120\n",
      "PO 125\n",
      "UN 5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# First we get the complete data set \n",
    "# Right I just have the first 6 months of the 2016 season.  \n",
    "# Each entry here is an array with two entries.  the first contains a simple feature\n",
    "# vector: [pitcher_hand, batter_hand, batter_pos],the second entry is a list of\n",
    "# pitch identifiers. Each identifier by a 2 letter code, each mapping to the \n",
    "# typical pitch name.  \n",
    "full_data = [] \n",
    "year = 2016\n",
    "for m in [3,4,5,6,7,8]:\n",
    "    fn = \"../data/pitches_{}_{}.p\".format(year, m)\n",
    "    seqs = pickle.load(open(fn, \"rb\"))\n",
    "    full_data += seqs\n",
    "\n",
    "# The MLBgame api documentation is incomplete, but from reading \n",
    "# the source code, there should be a total of 16 pitch types.   \n",
    "cleaned_data = [] # no 0 or 1 length sequences. \n",
    "longest_seq = 0\n",
    "empties_or_single = 0\n",
    "pitch_types = set()\n",
    "for line in full_data:\n",
    "    if(len(line[1]) > longest_seq): longest_seq = len(line[1])\n",
    "    if(len(line[1]) <= 1): \n",
    "        empties_or_single += 1\n",
    "    else:\n",
    "        cleaned_data.append(line[1])\n",
    "        for p in line[1]:# the seq is the second element, first is the feature vector\n",
    "            pitch_types.add(p)\n",
    "\n",
    "print(\"longest sequence length: {}\\nempties: {}\\ntotal (clean): {}\\npitch types: {}\".format(longest_seq, \n",
    "                                                                                            empties_or_single,\n",
    "                                                                                            len(cleaned_data),\n",
    "                                                                                            len(pitch_types)))\n",
    "pitch_counts = { p: 0 for p in pitch_types }\n",
    "for line in cleaned_data:\n",
    "    for p in line:\n",
    "        pitch_counts[p] += 1\n",
    "\n",
    "# To ensure the mappings make sense, lets count the pitch occurencesand compare them to\n",
    "# their actual names.\n",
    "for pitch, count in pitch_counts.items():\n",
    "    print(pitch, count)\n",
    "\n",
    "# Saving the cleaned data to a pickle to make it easier to work with the other models. \n",
    "pickle.dump(cleaned_data, open(\"../data/pitches_full_{}.p\".format(year), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "* 'SL' - 70932  -  slider\n",
    "* 'KC' - 9628   -  knuckle-curve\n",
    "* 'CH' - 48577  -  changeup\n",
    "* 'SI' - 34286  -  fastball (sinker)\n",
    "* 'FO' - 188    -  pitch-out\n",
    "* 'FS' - 7154   -  fastball \n",
    "* 'CU' - 40774  -  curveball\n",
    "* 'PO' - 126    -  pitch-out (would be better modeled with on-base info)\n",
    "* 'KN' - 1504   -  knuckleball\n",
    "* 'FF' - 164046 -  fastball (four-seam)\n",
    "* 'EP' - 14     -  eephus\n",
    "* 'IN' - 1853   -  intentional walk (again, maybe better modeled with on-base info)\n",
    "* 'SC' - 26     -  screwball\n",
    "* 'FT' - 62120  -  fastball (two-seam)\n",
    "* 'FC' - 23526  -  fastball (cutter)   \n",
    "* 'UN' - 5      -  unidentified (need to deal with this)\n",
    "```\n",
    "\n",
    "This is encouragining, the 'odd' pitch types, like unidentified, pitch-out, and eephus occur in very small numbers. Right now my focus is on getting a network graph correctly built, and training it properly, but a stretch goal could be to address the issues related to the 'meta strategy' pitches like the pitch-out and intentional walk. I'll keep them in for now, but I might filter them out, or regather data that includes the on-base info, and see if that helps.\n",
    "\n",
    "The goal of this investigation was to understand the shape of the input data. The sequences will be composed of vectors with length 16. Each entry will correspond to one of the above pitch identifiers. These sequences of 'one-hot' vectors must be created and padded to the length of the maximum length sequence (18).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 18\n"
     ]
    }
   ],
   "source": [
    "# Creating X - padded sequences of one-hots. Need a dictionary of pitch types to an index.\n",
    "pitch_map = {\n",
    "    'KC': 0,\n",
    "    'CH': 1,\n",
    "    'SL': 2,\n",
    "    'SI': 3,\n",
    "    'FO': 4,\n",
    "    'FS': 5,\n",
    "    'CU': 6,\n",
    "    'PO': 7,\n",
    "    'KN': 8,\n",
    "    'FF': 9,\n",
    "    'EP': 10,\n",
    "    'IN': 11,\n",
    "    'SC': 12,\n",
    "    'FT': 13,\n",
    "    'FC': 14,\n",
    "    'UN': 15\n",
    "}\n",
    "\n",
    "MAX_LENGTH = longest_seq\n",
    "\n",
    "def create_onehot(seq):\n",
    "    ret = []\n",
    "    i = 0\n",
    "    for p in seq:\n",
    "        p_oh = np.zeros((len(pitch_map),), dtype=np.float32)\n",
    "        p_oh[pitch_map[p]] = 1.0\n",
    "        ret.append(p_oh)\n",
    "        i += 1\n",
    "    for j in range(i, MAX_LENGTH):# Pad to length. \n",
    "        ret.append(np.zeros((len(pitch_map),), dtype=np.float32))\n",
    "    return ret\n",
    "\n",
    "def create_target(seq):\n",
    "    ret = []\n",
    "    i = 0\n",
    "    for p in seq[1:]:\n",
    "        ret.append(pitch_map[p])\n",
    "        i += 1\n",
    "    for j in range(i, MAX_LENGTH):\n",
    "        ret.append(0)\n",
    "    return ret\n",
    "\n",
    "X_full = [] # Sequences of onehots.\n",
    "y_full = [] # index of correct pitch in the one-hot, starting at X[1]\n",
    "for line in cleaned_data:\n",
    "    X_full.append(create_onehot(line))\n",
    "    y_full.append(create_target(line))\n",
    "\n",
    "    \n",
    "# these should be 18. Nice.\n",
    "print(len(X_full[0]), len(X_full[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The first pass of the model is a very basic RNN.  Once I can get this training any data, I'll focus on making this an actual architecture that can work.  That is, add things like multiple layers, or using a different cell (like the LSTM).  For now just trying to get the basic RNN cell working.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Assumptions about data:\n",
    "#  - X Padded to MAX_SIZE, with 0-vectors of size(pitch_types)\n",
    "#  - X only includes the pitch sequences. \n",
    "#  - Note: in the other models, each input in the seq will also have the additional feature \n",
    "#          vector for the at-bat.\n",
    "#  - y Padded to MAX_SIZE, with 0's.  (get length off of X, though)\n",
    "\n",
    "##### Construction Phase ###############\n",
    "NUM_INPUTS  = 16    # Size of the input vector (the number of possible pitch types)\n",
    "NUM_OUTPUTS = 16    # Want a pitch type out, so same size as input.\n",
    "NUM_NEURONS = 10     # Number of neurons inside the RNN cell.  \n",
    "MAX_SIZE    = 18    # the maximum size of a sequence.  Everything gets padded to this, and masked.\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "### RNN Graph\n",
    "# 0-vector padded sequences.  \n",
    "X = tf.placeholder( tf.float32, [BATCH_SIZE, MAX_SIZE, NUM_INPUTS] ) \n",
    "# y is X shifted to the left, but also converted to the *index* of the correct logit - for seq2seq loss.\n",
    "y = tf.placeholder( tf.int32, [BATCH_SIZE, MAX_SIZE] ) \n",
    "\n",
    "# Get a 1D Tensor to hold the 'true' length of each padded sequence in a batch\n",
    "collapsed_features = tf.sign(tf.reduce_max(tf.abs(X), 2)) # use max+abs to see what elements arent 0-vectors\n",
    "seq_len  = tf.cast( tf.reduce_sum(collapsed_features, 1), tf.int32 ) # Count the 1's to get length.\n",
    "seq_mask = tf.sequence_mask(seq_len, maxlen=MAX_SIZE, dtype=tf.float32) # Create a mask from these lengths\n",
    "\n",
    "basic_cell   = tf.contrib.rnn.BasicRNNCell( num_units=NUM_NEURONS )\n",
    "# output is shaped [BATCH_SIZE, MAX_LENGTH, NUM_NEURONS]\n",
    "outputs, states = tf.nn.dynamic_rnn( basic_cell, X, dtype=tf.float32, sequence_length=seq_len ) \n",
    "\n",
    "### Loss, Optimization, Training.  \n",
    "\n",
    "# seq2seq loss gets the loss by comparing the logits of the prediction \n",
    "# to the index of the correct label, given by y.  seq_mask is used to stop\n",
    "# unrolling the dynamic RNN at the correct spot in the padded sequence.  \n",
    "\n",
    "# NOTE: I think the error is here. Since the outputs are [BATCH_SIZE, MAX_LENGTH, NUM_NEURONS], they can't be\n",
    "#       directly used as a prediction.  Need to turn the output into a prediction by using another\n",
    "#       network layer that converts the [BATCH_SIZE, MAX_LENGTH, NUM_NEURONS] tensor into a \n",
    "#       [BATCH_SIZE, MAX_LENGTH, NUM_OUTPUTS] vector (logits for each pitch type.)\n",
    "# \n",
    "# Atleast, I think this is normally done with a fully connected layer between the outputs and the inputs to the\n",
    "# actual loss function. \n",
    "logits = tf.contrib.layers.fully_connected(outputs, NUM_OUTPUTS)\n",
    "\n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits, \n",
    "                                        y, \n",
    "                                        seq_mask, \n",
    "                                        average_across_timesteps=True, \n",
    "                                        average_across_batch=True)\n",
    "\n",
    "optimizer   = tf.train.AdamOptimizer( learning_rate=LEARNING_RATE )\n",
    "\n",
    "# Once we have a loss function, we can just let the optimizer do its job. (hopefully)\n",
    "training_op = optimizer.minimize( loss )\n",
    "\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Right now I'm still trying to get the network to actually process a batch of examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training Phase ###############\n",
    "EPOCHS     = 10 # Will need to figure out what this should be. dVC stuff?\n",
    "ITERATIONS = 100\n",
    "\n",
    "# NOTE: Hacky right now, but just want to get data into the model.\n",
    "# TODO: actually turn the data into tf.Dataset objects? \n",
    "# TODO: or use some of the batch operations?\n",
    "def get_training_batch(X, y, batch_size):\n",
    "    ids = np.random.randint(0, len(X), batch_size)\n",
    "    return np.array(X)[ids], np.array(y)[ids]\n",
    "\n",
    "# Testing for now, want to see if it actually updates on one batch.\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # For debugging\n",
    "    print(\"shape(seq_len): \", sess.run(seq_len, feed_dict={X: X_batch, y: y_batch}).shape)\n",
    "    print(\"shape(seq_mask): \", sess.run(seq_mask, feed_dict={X: X_batch, y: y_batch}).shape)\n",
    "    print(\"shape(outputs): \", sess.run(outputs, feed_dict={X: X_batch, y: y_batch}).shape)\n",
    "    print(\"shape(state): \", sess.run(states, feed_dict={X: X_batch, y: y_batch}).shape)\n",
    "    print(\"shape(logits): \", sess.run(logits, feed_dict={X: X_batch, y: y_batch}).shape)\n",
    "    \n",
    "    for i in range(ITERATIONS):\n",
    "        X_batch, y_batch = get_training_batch( X_full, y_full, BATCH_SIZE )\n",
    "        l = sess.run(loss, feed_dict={X: X_batch, y: y_batch})\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hand and Position Model\n",
    "\n",
    "This model will be similar to the previous one, but the vectors in the sequences will also include the pitch and position data from the at bat.  \n",
    "\n",
    "### Data\n",
    "First, the new example vectors need to be created.  The y tensor is exactly the same, but extra work needs to be done to create X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest sequence length: 18\n",
      "empties: 13757\n",
      "total (clean): 110216\n",
      "pitch types: 16\n",
      "pos types: 125\n",
      "{'DH-1B', '2B-SS', 'RF', 'CF-3B', 'PR-2B', 'LF-C', 'DH-3B', 'SS-1B', 'SS-3B-SS', 'C', '1B-2B', '1B-LF', '1B-P', 'SS-2B', 'LF-3B', 'PH-1B', 'DH-SS', '1B-3B', 'PR-DH-3B', 'CF-SS', '3B-P', 'LF-CF', '1B-CF', 'LF-2B', 'RF-LF-1B', 'PH-DH', 'DH-2B', '3B-RF', 'PR-RF', 'LF-RF-LF', '1B-RF', '2B-CF', '3B-RF-2B', 'CF-2B', 'C-3B', 'DH-LF', 'PH-3B-1B', '2B-1B', 'SS-LF', 'SS-P', '2B-LF-RF', 'LF-SS', 'LF-P-LF-P', 'PH-LF-RF', 'PH-2B', '2B-RF', 'SS-3B', 'PR-LF-CF', '3B-RF-LF', 'PH-LF-CF', 'PH-DH-RF', 'C-LF-P-2B', 'PR-1B', '3B-2B', 'PH-RF-LF', 'CF-LF-CF', '2B-3B-LF', 'PH-SS', 'RF-CF', '2B', 'RF-LF', '3B-RF-3B', 'RF-LF-3B', 'SS', 'DH', '3B-LF', 'PH-3B', '3B-2B-LF', 'PH-DH-2B', 'PR-RF-CF', 'LF', '3B-SS', 'RF-SS', 'RF-LF-CF', '2B-3B', '1B-C', 'P-LF-P', 'PH-CF', 'C-1B', 'PH', 'P', 'LF-1B-LF', 'PH-C', 'PR-3B-1B', 'PH-2B-1B', 'PR-DH', 'LF-CF-LF', 'LF-1B', 'CF-1B', '1B-SS', 'PH-1B-LF', 'LF-RF', 'PR-1B-LF-1B', 'SS-RF', '2B-P', 'C-LF', 'CF-LF', '3B-LF-3B', 'DH-RF', 'PR-DH-LF', 'PR-CF', 'PR-1B-2B', 'PR-LF', 'RF-3B', '3B-CF', '2B-LF-3B', 'DH-C', 'RF-1B-LF', '1B', 'PH-1B-RF', 'PR-C', '3B', 'RF-1B', 'CF-RF', 'PR-3B', 'PH-LF', '2B-LF', '3B-1B-3B', 'CF', '3B-1B', 'PR-SS', 'PH-1B-2B', 'PH-RF', 'RF-2B', 'PH-P'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    " \n",
    "full_data = [] \n",
    "year = 2016\n",
    "for m in [3,4,5,6,7,8]:\n",
    "    fn = \"../data/pitches_{}_{}.p\".format(year, m)\n",
    "    seqs = pickle.load(open(fn, \"rb\"))\n",
    "    full_data += seqs\n",
    "\n",
    "cleaned_data = [] # no 0 or 1 length sequences. \n",
    "longest_seq = 0\n",
    "empties_or_single = 0\n",
    "pitch_types = set()\n",
    "pos_types   = set()\n",
    "\n",
    "for line in full_data:\n",
    "    if(len(line[1]) > longest_seq): longest_seq = len(line[1])\n",
    "    if(len(line[1]) <= 1): \n",
    "        empties_or_single += 1\n",
    "    else:\n",
    "        cleaned_data.append(line)\n",
    "        pos_types.add(line[0][2]) \n",
    "        for p in line[1]: # the seq is the second element, first is the feature vector\n",
    "            pitch_types.add(p)\n",
    "\n",
    "print(\"longest sequence length: {}\\nempties: {}\\ntotal (clean): {}\\npitch types: {}\".format(longest_seq, \n",
    "                                                                                            empties_or_single,\n",
    "                                                                                            len(cleaned_data),\n",
    "                                                                                            len(pitch_types)))\n",
    "\n",
    "print(\"pos types: {}\".format(len(pos_types)))\n",
    "# Saving the cleaned data to a pickle to make it easier to work with the other models. \n",
    "pickle.dump(cleaned_data, open(\"../data/pitches_full_{}.p\".format(year), \"wb\"))\n",
    "\n",
    "print(pos_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woa! Lot more positions than I was anticipating.  Looks like they allow for multiple positions.  I think I can still handle this, but the positions feature will have to be a one-hot of all the individual positions, and a batters pos vector would contain a value for each position listed.  These could be normalized, even. \n",
    "\n",
    "Regardless, the position will be a one-hot vector of length 12.  This will be concatenated with each pitches onehot vector.  In addition, two other values will be added.  Two 0/1 values that represent the handedness of the batter and pitcher.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 {'DH', 'CF', 'C', 'RF', 'P', 'PH', '3B', 'PR', '2B', 'LF', '1B', 'SS'}\n"
     ]
    }
   ],
   "source": [
    "simple_poss = set()\n",
    "for p in pos_types:\n",
    "    p_split = p.split(\"-\")\n",
    "    for i in p_split:\n",
    "        simple_poss.add(i)\n",
    "# This should just be the regular list of positions. \n",
    "print(len(simple_poss), simple_poss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['R', 'R', 'RF'], ['FT', 'FF', 'CH']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len = 18\n",
      "feature_len = 14\n"
     ]
    }
   ],
   "source": [
    "# Creating X - padded sequences of one-hots. Need a dictionary of pitch types and positions.\n",
    "pitch_map = {\n",
    "    'KC': 0,\n",
    "    'CH': 1,\n",
    "    'SL': 2,\n",
    "    'SI': 3,\n",
    "    'FO': 4,\n",
    "    'FS': 5,\n",
    "    'CU': 6,\n",
    "    'PO': 7,\n",
    "    'KN': 8,\n",
    "    'FF': 9,\n",
    "    'EP': 10,\n",
    "    'IN': 11,\n",
    "    'SC': 12,\n",
    "    'FT': 13,\n",
    "    'FC': 14,\n",
    "    'UN': 15\n",
    "}\n",
    "\n",
    "pos_map = {\n",
    "    '1B': 0,\n",
    "    '2B': 1,\n",
    "    '3B': 2,\n",
    "    'PR': 3,\n",
    "    'P':  4,\n",
    "    'C':  5,\n",
    "    'DH': 6,\n",
    "    'SS': 7,\n",
    "    'PH': 8,\n",
    "    'CF': 9,\n",
    "    'RF': 10,\n",
    "    'LF': 11\n",
    "}\n",
    "\n",
    "# Need to think about handling the 'both' pitchers and 'switch' hitters. \n",
    "hand_map ={\n",
    "    'L': 0.0,\n",
    "    'R': 1.0,\n",
    "    'B': 0.5, # Just doing this for now. \n",
    "    'S': 0.5\n",
    "}\n",
    "\n",
    "MAX_LENGTH = longest_seq\n",
    "NUM_EXTRA_FEATURES = len(pos_map)+2 # pitcher hand, batter hand, batter pos\n",
    "\n",
    "def create_feature_vec(seq):\n",
    "    # Create positions one-hot\n",
    "    pos = np.zeros((len(pos_map),), dtype=np.float32)\n",
    "    for p in seq[0][2].split(\"-\"):\n",
    "        pos[pos_map[p]] = 1.0\n",
    "    \n",
    "    # Handedness features\n",
    "    hands = np.zeros((2,), dtype=np.float32)\n",
    "    hands[0] = hand_map[seq[0][0]]\n",
    "    hands[1] = hand_map[seq[0][1]]\n",
    "    return np.concatenate((hands, pos))\n",
    "    \n",
    "def create_oneshot_seq(seq):\n",
    "    # Pitch Sequence\n",
    "    pitches = []\n",
    "    i = 0\n",
    "    for p in seq[1]:\n",
    "        p_oh = np.zeros((len(pitch_map),), dtype=np.float32)\n",
    "        p_oh[pitch_map[p]] = 1.0\n",
    "        pitches.append(p_oh)\n",
    "        i += 1\n",
    "    for j in range(i, MAX_LENGTH):# Pad to length. \n",
    "        pitches.append(np.zeros((len(pitch_map)), dtype=np.float32))\n",
    "    return np.array(pitches)\n",
    "\n",
    "def create_target(seq):\n",
    "    ret = []\n",
    "    i = 0\n",
    "    for p in seq[1][1:]:\n",
    "        ret.append(pitch_map[p])\n",
    "        i += 1\n",
    "    for j in range(i, MAX_LENGTH):\n",
    "        ret.append(0)\n",
    "    return ret\n",
    "\n",
    "X_full = [] # Sequences of onehots.\n",
    "f_full = [] # Feature vectors\n",
    "y_full = [] # index of correct pitch in the one-hot, starting at X[1]\n",
    "for line in cleaned_data:\n",
    "    X_full.append(create_oneshot_seq(line))\n",
    "    f_full.append(create_feature_vec(line))\n",
    "    y_full.append(create_target(line))\n",
    "\n",
    "# Each entry in X_full will be a 2 entry array.  The first element is the feature vector and the\n",
    "# second entry is the actual pitch sequence.\n",
    "\n",
    "# The sequence should have the same length as 'max length'\n",
    "# the feature vector should be 12 + 2 = 14 (num_pos + num_hands)\n",
    "\n",
    "print(\"seq_len = {}\\nfeature_len = {}\".format(len(X_full[0]), len(f_full[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "I feel like there is an issue with just extending the tensor that goes from cell to cell in the RNN.  If at each iteration, we get an output that represents the logits for each feature in the feature vector, whats to stop the network from just predicting the handedness and position at each step, because it never changes?\n",
    "\n",
    "Is there a way to restrict the calculation of the logits to just the 16 pitch outputs?  Could I make the input be the 16+2+12 vector, make the internals output 16, and the next input would be output+(2+12features)?\n",
    "\n",
    "Can I force the prediction to just be the result of a FC from a subset of output tensor?  That seems like the right direction. Mask, or create a new tensor, then change the prediction op to use that.  Then the loss stuff would stay the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Assumptions about data:\n",
    "#  - X Padded to MAX_SIZE, with 0-vectors of size(pitch_types)\n",
    "#  - X also includes the additional hand and pos feature vector. \n",
    "#  - y Padded to MAX_SIZE, with 0's.  (get length off of X, though)\n",
    "\n",
    "##### Construction Phase ###############\n",
    "NUM_INPUTS   = 16    # Size of the input vector (the number of possible pitch types)\n",
    "NUM_OUTPUTS  = 16    # Want a pitch type out, so same size as input.\n",
    "NUM_NEURONS  = 10    # Number of neurons inside the RNN cell.  \n",
    "MAX_SIZE     = 18    # the maximum size of a sequence.  Everything gets padded to this, and masked.\n",
    "FEATURE_SIZE = 14    # Size of the additional feature vector.\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "LEARNING_RATE = 0.015\n",
    "\n",
    "### RNN Graph\n",
    "X = tf.placeholder( tf.float32, [BATCH_SIZE, MAX_SIZE, NUM_INPUTS] )\n",
    "F = tf.placeholder( tf.float32, [BATCH_SIZE, FEATURE_SIZE] )\n",
    "\n",
    "# y is X shifted to the left, but also converted to the *index* of the correct logit - for seq2seq loss.\n",
    "y = tf.placeholder( tf.int32, [BATCH_SIZE, MAX_SIZE] ) \n",
    "\n",
    "# Get a 1D Tensor to hold the 'true' length of each padded sequence in a batch\n",
    "collapsed_features = tf.sign(tf.reduce_max(tf.abs(X), 2)) # use max+abs to see what elements arent 0-vectors\n",
    "seq_len  = tf.cast( tf.reduce_sum(collapsed_features, 1), tf.int32 ) # Count the 1's to get length.\n",
    "seq_mask = tf.sequence_mask(seq_len, maxlen=MAX_SIZE, dtype=tf.float32) # Create a mask from these lengths\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell( num_units=NUM_NEURONS )\n",
    "outputs, states = tf.nn.dynamic_rnn( basic_cell, X, dtype=tf.float32, sequence_length=seq_len ) \n",
    "\n",
    "# Here's the difference in implementation.  The logits are computed from a concatenation of the\n",
    "# outputs and the 'static' feature vector.  \n",
    "\n",
    "F_expanded = tf.tile(tf.expand_dims(F, 1), [1, MAX_SIZE, 1])\n",
    "\n",
    "combined_outputs = tf.concat((outputs, F_expanded), 2)\n",
    "\n",
    "# combined_output = tf.SOMETHING.concatenate( outputs, features )\n",
    "logits = tf.contrib.layers.fully_connected(combined_outputs, NUM_OUTPUTS)\n",
    "# logits = tf.contrib.layers.fully_connected(outputs, NUM_OUTPUTS)\n",
    "\n",
    "\n",
    "### Loss, Optimization, Training.  \n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits, \n",
    "                                        y, \n",
    "                                        seq_mask, \n",
    "                                        average_across_timesteps=True, \n",
    "                                        average_across_batch=True)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Once we have a loss function, we can just let the optimizer do its job. (hopefully)\n",
    "training_op = optimizer.minimize( loss )\n",
    "\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(F):  (5, 14)\n",
      "shape(tiled_features):  (5, 18, 24)\n",
      "loss at i 0: 2.6451919078826904\n",
      "loss at i 10: 2.42616605758667\n",
      "loss at i 20: 2.3113667964935303\n",
      "loss at i 30: 2.2893896102905273\n",
      "loss at i 40: 1.9962695837020874\n",
      "loss at i 50: 2.0949418544769287\n",
      "loss at i 60: 2.1340460777282715\n",
      "loss at i 70: 1.9927934408187866\n",
      "loss at i 80: 1.7872798442840576\n",
      "loss at i 90: 1.6347181797027588\n",
      "loss at i 100: 1.7173080444335938\n",
      "loss at i 110: 1.958595871925354\n",
      "loss at i 120: 1.9544323682785034\n",
      "loss at i 130: 2.432661771774292\n",
      "loss at i 140: 1.4367483854293823\n",
      "loss at i 150: 1.9806965589523315\n",
      "loss at i 160: 1.9730268716812134\n",
      "loss at i 170: 1.9884119033813477\n",
      "loss at i 180: 1.9526020288467407\n",
      "loss at i 190: 1.9420955181121826\n",
      "loss at i 200: 2.107860803604126\n",
      "loss at i 210: 1.5580101013183594\n",
      "loss at i 220: 1.5647457838058472\n",
      "loss at i 230: 1.9798862934112549\n",
      "loss at i 240: 1.6896913051605225\n",
      "loss at i 250: 1.7287826538085938\n",
      "loss at i 260: 1.4648513793945312\n",
      "loss at i 270: 1.7118891477584839\n",
      "loss at i 280: 2.3962113857269287\n",
      "loss at i 290: 1.7284729480743408\n",
      "loss at i 300: 1.7461585998535156\n",
      "loss at i 310: 1.6018269062042236\n",
      "loss at i 320: 2.163527488708496\n",
      "loss at i 330: 2.007636308670044\n",
      "loss at i 340: 2.4854485988616943\n",
      "loss at i 350: 1.248206615447998\n",
      "loss at i 360: 2.4552605152130127\n",
      "loss at i 370: 1.7624950408935547\n",
      "loss at i 380: 1.6888799667358398\n",
      "loss at i 390: 1.6326848268508911\n",
      "loss at i 400: 2.1823339462280273\n",
      "loss at i 410: 1.4213849306106567\n",
      "loss at i 420: 2.0874109268188477\n",
      "loss at i 430: 2.956364631652832\n",
      "loss at i 440: 1.701833724975586\n",
      "loss at i 450: 2.0747411251068115\n",
      "loss at i 460: 1.3780723810195923\n",
      "loss at i 470: 1.743080973625183\n",
      "loss at i 480: 1.457770586013794\n",
      "loss at i 490: 1.2622112035751343\n",
      "loss at i 500: 1.6232713460922241\n",
      "loss at i 510: 1.7230453491210938\n",
      "loss at i 520: 1.6463836431503296\n",
      "loss at i 530: 1.778882384300232\n",
      "loss at i 540: 1.3977081775665283\n",
      "loss at i 550: 1.7406352758407593\n",
      "loss at i 560: 2.208164930343628\n",
      "loss at i 570: 1.945770263671875\n",
      "loss at i 580: 1.2146120071411133\n",
      "loss at i 590: 2.0587430000305176\n",
      "loss at i 600: 2.549851655960083\n",
      "loss at i 610: 1.4820091724395752\n",
      "loss at i 620: 1.345605492591858\n",
      "loss at i 630: 1.8078969717025757\n",
      "loss at i 640: 1.6777100563049316\n",
      "loss at i 650: 1.790365219116211\n",
      "loss at i 660: 1.6796820163726807\n",
      "loss at i 670: 1.9779694080352783\n",
      "loss at i 680: 1.5737450122833252\n",
      "loss at i 690: 1.2133536338806152\n",
      "loss at i 700: 1.6403777599334717\n",
      "loss at i 710: 2.489032506942749\n",
      "loss at i 720: 1.880233883857727\n",
      "loss at i 730: 1.7830349206924438\n",
      "loss at i 740: 1.5154099464416504\n",
      "loss at i 750: 1.8017024993896484\n",
      "loss at i 760: 1.5887717008590698\n",
      "loss at i 770: 1.9817436933517456\n",
      "loss at i 780: 1.51605224609375\n",
      "loss at i 790: 2.1420061588287354\n",
      "loss at i 800: 2.5812270641326904\n",
      "loss at i 810: 1.7558114528656006\n",
      "loss at i 820: 1.9414634704589844\n",
      "loss at i 830: 1.4026249647140503\n",
      "loss at i 840: 1.6524592638015747\n",
      "loss at i 850: 1.9672181606292725\n",
      "loss at i 860: 1.3537406921386719\n",
      "loss at i 870: 1.5398973226547241\n",
      "loss at i 880: 1.5907694101333618\n",
      "loss at i 890: 1.8163032531738281\n",
      "loss at i 900: 1.8825857639312744\n",
      "loss at i 910: 1.4353277683258057\n",
      "loss at i 920: 1.5744571685791016\n",
      "loss at i 930: 1.914144515991211\n",
      "loss at i 940: 2.1656181812286377\n",
      "loss at i 950: 2.145761728286743\n",
      "loss at i 960: 1.7805598974227905\n",
      "loss at i 970: 1.6309521198272705\n",
      "loss at i 980: 1.6936192512512207\n",
      "loss at i 990: 1.9383265972137451\n",
      "loss at i 1000: 2.1178483963012695\n",
      "loss at i 1010: 2.209892511367798\n",
      "loss at i 1020: 2.172776937484741\n",
      "loss at i 1030: 2.1439483165740967\n",
      "loss at i 1040: 2.057093620300293\n",
      "loss at i 1050: 1.4750936031341553\n",
      "loss at i 1060: 1.9369564056396484\n",
      "loss at i 1070: 2.1303319931030273\n",
      "loss at i 1080: 1.8315925598144531\n",
      "loss at i 1090: 1.4281448125839233\n",
      "loss at i 1100: 1.7605417966842651\n",
      "loss at i 1110: 1.9956259727478027\n",
      "loss at i 1120: 2.030576467514038\n",
      "loss at i 1130: 1.8063147068023682\n",
      "loss at i 1140: 1.5920687913894653\n",
      "loss at i 1150: 1.6324493885040283\n",
      "loss at i 1160: 1.2171915769577026\n",
      "loss at i 1170: 1.8091599941253662\n",
      "loss at i 1180: 1.463642954826355\n",
      "loss at i 1190: 1.5829544067382812\n",
      "loss at i 1200: 1.742396354675293\n",
      "loss at i 1210: 1.664230465888977\n",
      "loss at i 1220: 1.5262713432312012\n",
      "loss at i 1230: 1.6500962972640991\n",
      "loss at i 1240: 1.8541582822799683\n",
      "loss at i 1250: 1.724664330482483\n",
      "loss at i 1260: 2.1227431297302246\n",
      "loss at i 1270: 1.751554250717163\n",
      "loss at i 1280: 1.8976538181304932\n",
      "loss at i 1290: 1.304152250289917\n",
      "loss at i 1300: 1.5643267631530762\n",
      "loss at i 1310: 1.7578507661819458\n",
      "loss at i 1320: 1.7796076536178589\n",
      "loss at i 1330: 1.6940805912017822\n",
      "loss at i 1340: 1.579182744026184\n",
      "loss at i 1350: 1.5217139720916748\n",
      "loss at i 1360: 1.9088029861450195\n",
      "loss at i 1370: 1.5707191228866577\n",
      "loss at i 1380: 1.7839157581329346\n",
      "loss at i 1390: 1.6251732110977173\n",
      "loss at i 1400: 1.9303324222564697\n",
      "loss at i 1410: 1.6356714963912964\n",
      "loss at i 1420: 2.3131065368652344\n",
      "loss at i 1430: 1.155335783958435\n",
      "loss at i 1440: 1.7948657274246216\n",
      "loss at i 1450: 1.7243187427520752\n",
      "loss at i 1460: 2.2485337257385254\n",
      "loss at i 1470: 1.8609910011291504\n",
      "loss at i 1480: 1.7344698905944824\n",
      "loss at i 1490: 1.9947835206985474\n",
      "loss at i 1500: 2.03536057472229\n",
      "loss at i 1510: 2.026765823364258\n",
      "loss at i 1520: 1.9187902212142944\n",
      "loss at i 1530: 1.77910578250885\n",
      "loss at i 1540: 1.50916588306427\n",
      "loss at i 1550: 1.6905957460403442\n",
      "loss at i 1560: 1.7364355325698853\n",
      "loss at i 1570: 1.565304160118103\n",
      "loss at i 1580: 1.723301887512207\n",
      "loss at i 1590: 1.6125110387802124\n",
      "loss at i 1600: 1.8568998575210571\n",
      "loss at i 1610: 1.9769872426986694\n",
      "loss at i 1620: 1.6669204235076904\n",
      "loss at i 1630: 1.5841470956802368\n",
      "loss at i 1640: 1.712792992591858\n",
      "loss at i 1650: 1.5506693124771118\n",
      "loss at i 1660: 2.139817714691162\n",
      "loss at i 1670: 2.242144823074341\n",
      "loss at i 1680: 1.9229753017425537\n",
      "loss at i 1690: 1.5191210508346558\n",
      "loss at i 1700: 1.7522979974746704\n",
      "loss at i 1710: 1.5091803073883057\n",
      "loss at i 1720: 1.7827608585357666\n",
      "loss at i 1730: 1.999853253364563\n",
      "loss at i 1740: 2.101137161254883\n",
      "loss at i 1750: 1.3398319482803345\n",
      "loss at i 1760: 1.4483109712600708\n",
      "loss at i 1770: 1.638877034187317\n",
      "loss at i 1780: 1.7363500595092773\n",
      "loss at i 1790: 1.4925974607467651\n",
      "loss at i 1800: 1.6568522453308105\n",
      "loss at i 1810: 1.6556029319763184\n",
      "loss at i 1820: 1.6713294982910156\n",
      "loss at i 1830: 1.7642428874969482\n",
      "loss at i 1840: 1.5699801445007324\n",
      "loss at i 1850: 1.5766431093215942\n",
      "loss at i 1860: 1.875993251800537\n",
      "loss at i 1870: 2.0091471672058105\n",
      "loss at i 1880: 1.4681510925292969\n",
      "loss at i 1890: 1.8848063945770264\n",
      "loss at i 1900: 1.6424931287765503\n",
      "loss at i 1910: 2.0554890632629395\n",
      "loss at i 1920: 1.2016668319702148\n",
      "loss at i 1930: 2.068084478378296\n",
      "loss at i 1940: 2.1384782791137695\n",
      "loss at i 1950: 1.8808014392852783\n",
      "loss at i 1960: 2.2002992630004883\n",
      "loss at i 1970: 1.861858606338501\n",
      "loss at i 1980: 1.555140733718872\n",
      "loss at i 1990: 1.9112167358398438\n",
      "loss at i 2000: 2.0499000549316406\n",
      "loss at i 2010: 2.027611494064331\n",
      "loss at i 2020: 1.8074073791503906\n",
      "loss at i 2030: 1.645351767539978\n",
      "loss at i 2040: 1.3859503269195557\n",
      "loss at i 2050: 1.6709126234054565\n",
      "loss at i 2060: 1.994226098060608\n",
      "loss at i 2070: 2.3284189701080322\n",
      "loss at i 2080: 1.6090868711471558\n",
      "loss at i 2090: 1.9094191789627075\n",
      "loss at i 2100: 2.1005489826202393\n",
      "loss at i 2110: 1.5196504592895508\n",
      "loss at i 2120: 2.2114391326904297\n",
      "loss at i 2130: 1.2895276546478271\n",
      "loss at i 2140: 2.176865339279175\n",
      "loss at i 2150: 2.162503242492676\n",
      "loss at i 2160: 2.1056597232818604\n",
      "loss at i 2170: 1.5810010433197021\n",
      "loss at i 2180: 1.790685772895813\n",
      "loss at i 2190: 1.346091628074646\n",
      "loss at i 2200: 1.6680068969726562\n",
      "loss at i 2210: 1.367570161819458\n",
      "loss at i 2220: 1.9494514465332031\n",
      "loss at i 2230: 1.9402008056640625\n",
      "loss at i 2240: 1.2281500101089478\n",
      "loss at i 2250: 1.614223599433899\n",
      "loss at i 2260: 2.206242322921753\n",
      "loss at i 2270: 1.8905858993530273\n",
      "loss at i 2280: 1.284940481185913\n",
      "loss at i 2290: 2.049295663833618\n",
      "loss at i 2300: 1.4819762706756592\n",
      "loss at i 2310: 1.7477617263793945\n",
      "loss at i 2320: 1.6516445875167847\n",
      "loss at i 2330: 1.579390525817871\n",
      "loss at i 2340: 1.7993435859680176\n",
      "loss at i 2350: 2.024449586868286\n",
      "loss at i 2360: 1.9108922481536865\n",
      "loss at i 2370: 1.8363691568374634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at i 2380: 1.5496885776519775\n",
      "loss at i 2390: 1.8234124183654785\n",
      "loss at i 2400: 1.6629376411437988\n",
      "loss at i 2410: 1.4765913486480713\n",
      "loss at i 2420: 1.8025481700897217\n",
      "loss at i 2430: 1.4280924797058105\n",
      "loss at i 2440: 1.5513323545455933\n",
      "loss at i 2450: 1.6918102502822876\n",
      "loss at i 2460: 1.7705575227737427\n",
      "loss at i 2470: 1.5357840061187744\n",
      "loss at i 2480: 1.798229455947876\n",
      "loss at i 2490: 2.1960113048553467\n",
      "loss at i 2500: 1.801081895828247\n",
      "loss at i 2510: 2.2344343662261963\n",
      "loss at i 2520: 1.4584532976150513\n",
      "loss at i 2530: 1.6940016746520996\n",
      "loss at i 2540: 1.450158715248108\n",
      "loss at i 2550: 1.6697673797607422\n",
      "loss at i 2560: 2.269953489303589\n",
      "loss at i 2570: 1.6179018020629883\n",
      "loss at i 2580: 2.014347791671753\n",
      "loss at i 2590: 1.7201809883117676\n",
      "loss at i 2600: 2.14612078666687\n",
      "loss at i 2610: 1.8518308401107788\n",
      "loss at i 2620: 1.2271506786346436\n",
      "loss at i 2630: 1.8853482007980347\n",
      "loss at i 2640: 2.1139519214630127\n",
      "loss at i 2650: 1.5687170028686523\n",
      "loss at i 2660: 1.4854536056518555\n",
      "loss at i 2670: 1.4011030197143555\n",
      "loss at i 2680: 1.5966943502426147\n",
      "loss at i 2690: 2.059840679168701\n",
      "loss at i 2700: 2.057888984680176\n",
      "loss at i 2710: 2.1772701740264893\n",
      "loss at i 2720: 1.4177201986312866\n",
      "loss at i 2730: 1.676068902015686\n",
      "loss at i 2740: 1.576936960220337\n",
      "loss at i 2750: 2.2652781009674072\n",
      "loss at i 2760: 1.8490976095199585\n",
      "loss at i 2770: 1.4391895532608032\n",
      "loss at i 2780: 1.4382380247116089\n",
      "loss at i 2790: 1.2776674032211304\n",
      "loss at i 2800: 1.6109539270401\n",
      "loss at i 2810: 1.5413297414779663\n",
      "loss at i 2820: 1.2294567823410034\n",
      "loss at i 2830: 2.0942118167877197\n",
      "loss at i 2840: 1.6160857677459717\n",
      "loss at i 2850: 1.5530353784561157\n",
      "loss at i 2860: 1.5559093952178955\n",
      "loss at i 2870: 1.5325469970703125\n",
      "loss at i 2880: 1.9091413021087646\n",
      "loss at i 2890: 1.2246536016464233\n",
      "loss at i 2900: 1.7155505418777466\n",
      "loss at i 2910: 1.4014036655426025\n",
      "loss at i 2920: 1.7696747779846191\n",
      "loss at i 2930: 2.1872036457061768\n",
      "loss at i 2940: 1.6990089416503906\n",
      "loss at i 2950: 1.6783126592636108\n",
      "loss at i 2960: 2.258352279663086\n",
      "loss at i 2970: 1.624290943145752\n",
      "loss at i 2980: 1.2688482999801636\n",
      "loss at i 2990: 1.6493161916732788\n",
      "loss at i 3000: 1.8999398946762085\n",
      "loss at i 3010: 1.647430658340454\n",
      "loss at i 3020: 1.745036244392395\n",
      "loss at i 3030: 1.4086658954620361\n",
      "loss at i 3040: 2.028733730316162\n",
      "loss at i 3050: 1.7343201637268066\n",
      "loss at i 3060: 1.3917357921600342\n",
      "loss at i 3070: 1.6004769802093506\n",
      "loss at i 3080: 1.8828881978988647\n",
      "loss at i 3090: 1.4832162857055664\n",
      "loss at i 3100: 1.6166472434997559\n",
      "loss at i 3110: 1.6637694835662842\n",
      "loss at i 3120: 1.442055344581604\n",
      "loss at i 3130: 2.0136170387268066\n",
      "loss at i 3140: 2.0547637939453125\n",
      "loss at i 3150: 1.6036618947982788\n",
      "loss at i 3160: 1.6481081247329712\n",
      "loss at i 3170: 1.5812677145004272\n",
      "loss at i 3180: 1.5674375295639038\n",
      "loss at i 3190: 1.6286495923995972\n",
      "loss at i 3200: 1.630192518234253\n",
      "loss at i 3210: 1.641470193862915\n",
      "loss at i 3220: 1.694989800453186\n",
      "loss at i 3230: 2.203141450881958\n",
      "loss at i 3240: 1.5571798086166382\n",
      "loss at i 3250: 1.5195698738098145\n",
      "loss at i 3260: 1.8338083028793335\n",
      "loss at i 3270: 1.94014310836792\n",
      "loss at i 3280: 1.8523268699645996\n",
      "loss at i 3290: 1.5756909847259521\n",
      "loss at i 3300: 1.5402392148971558\n",
      "loss at i 3310: 1.6298633813858032\n",
      "loss at i 3320: 1.136377215385437\n",
      "loss at i 3330: 1.7980893850326538\n",
      "loss at i 3340: 1.6085472106933594\n",
      "loss at i 3350: 1.9257041215896606\n",
      "loss at i 3360: 1.7066354751586914\n",
      "loss at i 3370: 1.3861664533615112\n",
      "loss at i 3380: 2.2502248287200928\n",
      "loss at i 3390: 1.6516773700714111\n",
      "loss at i 3400: 1.3137832880020142\n",
      "loss at i 3410: 1.5800442695617676\n",
      "loss at i 3420: 1.7499858140945435\n",
      "loss at i 3430: 2.2268075942993164\n",
      "loss at i 3440: 2.168334722518921\n",
      "loss at i 3450: 1.8907815217971802\n",
      "loss at i 3460: 2.1178340911865234\n",
      "loss at i 3470: 2.0032730102539062\n",
      "loss at i 3480: 1.5375587940216064\n",
      "loss at i 3490: 1.727569341659546\n",
      "loss at i 3500: 1.4572383165359497\n",
      "loss at i 3510: 2.201253890991211\n",
      "loss at i 3520: 1.5194002389907837\n",
      "loss at i 3530: 1.6673415899276733\n",
      "loss at i 3540: 1.8312580585479736\n",
      "loss at i 3550: 2.0558784008026123\n",
      "loss at i 3560: 1.2376970052719116\n",
      "loss at i 3570: 1.649347186088562\n",
      "loss at i 3580: 1.603285551071167\n",
      "loss at i 3590: 1.9114792346954346\n",
      "loss at i 3600: 1.5322446823120117\n",
      "loss at i 3610: 1.649375081062317\n",
      "loss at i 3620: 1.7938448190689087\n",
      "loss at i 3630: 1.4260430335998535\n",
      "loss at i 3640: 1.746628761291504\n",
      "loss at i 3650: 1.7362045049667358\n",
      "loss at i 3660: 1.5265895128250122\n",
      "loss at i 3670: 1.8621418476104736\n",
      "loss at i 3680: 1.4376977682113647\n",
      "loss at i 3690: 1.7846013307571411\n",
      "loss at i 3700: 1.5924031734466553\n",
      "loss at i 3710: 1.9178820848464966\n",
      "loss at i 3720: 1.7315503358840942\n",
      "loss at i 3730: 2.2385573387145996\n",
      "loss at i 3740: 1.9809213876724243\n",
      "loss at i 3750: 1.6693979501724243\n",
      "loss at i 3760: 1.674456238746643\n",
      "loss at i 3770: 1.9429255723953247\n",
      "loss at i 3780: 1.6628060340881348\n",
      "loss at i 3790: 1.6627557277679443\n",
      "loss at i 3800: 1.3179168701171875\n",
      "loss at i 3810: 1.4625608921051025\n",
      "loss at i 3820: 1.3835747241973877\n",
      "loss at i 3830: 1.7293155193328857\n",
      "loss at i 3840: 1.6549689769744873\n",
      "loss at i 3850: 1.5965701341629028\n",
      "loss at i 3860: 2.252011775970459\n",
      "loss at i 3870: 1.6867977380752563\n",
      "loss at i 3880: 1.5693782567977905\n",
      "loss at i 3890: 1.813596248626709\n",
      "loss at i 3900: 1.3181040287017822\n",
      "loss at i 3910: 1.4785597324371338\n",
      "loss at i 3920: 1.6583876609802246\n",
      "loss at i 3930: 2.397655963897705\n",
      "loss at i 3940: 1.6665371656417847\n",
      "loss at i 3950: 2.0975356101989746\n",
      "loss at i 3960: 1.7112616300582886\n",
      "loss at i 3970: 2.218249797821045\n",
      "loss at i 3980: 1.604164481163025\n",
      "loss at i 3990: 1.769702672958374\n",
      "loss at i 4000: 1.76579749584198\n",
      "loss at i 4010: 1.5943105220794678\n",
      "loss at i 4020: 2.174978256225586\n",
      "loss at i 4030: 1.6468615531921387\n",
      "loss at i 4040: 1.4245322942733765\n",
      "loss at i 4050: 2.3173277378082275\n",
      "loss at i 4060: 1.5218054056167603\n",
      "loss at i 4070: 1.585464596748352\n",
      "loss at i 4080: 1.6171389818191528\n",
      "loss at i 4090: 2.244065284729004\n",
      "loss at i 4100: 1.6430283784866333\n",
      "loss at i 4110: 2.1919474601745605\n",
      "loss at i 4120: 1.5411157608032227\n",
      "loss at i 4130: 2.0228214263916016\n",
      "loss at i 4140: 2.088196277618408\n",
      "loss at i 4150: 1.8699021339416504\n",
      "loss at i 4160: 2.0316991806030273\n",
      "loss at i 4170: 1.696022629737854\n",
      "loss at i 4180: 1.9428308010101318\n",
      "loss at i 4190: 1.6792901754379272\n",
      "loss at i 4200: 1.4091204404830933\n",
      "loss at i 4210: 1.6003729104995728\n",
      "loss at i 4220: 1.4796674251556396\n",
      "loss at i 4230: 1.2645899057388306\n",
      "loss at i 4240: 1.4128049612045288\n",
      "loss at i 4250: 1.6836106777191162\n",
      "loss at i 4260: 1.4890692234039307\n",
      "loss at i 4270: 1.4471231698989868\n",
      "loss at i 4280: 1.6901072263717651\n",
      "loss at i 4290: 1.490849494934082\n",
      "loss at i 4300: 2.2903828620910645\n",
      "loss at i 4310: 1.9220761060714722\n",
      "loss at i 4320: 1.378785252571106\n",
      "loss at i 4330: 1.5121464729309082\n",
      "loss at i 4340: 1.4630171060562134\n",
      "loss at i 4350: 1.9196134805679321\n",
      "loss at i 4360: 1.3565049171447754\n",
      "loss at i 4370: 1.8365129232406616\n",
      "loss at i 4380: 2.190595865249634\n",
      "loss at i 4390: 1.3861480951309204\n",
      "loss at i 4400: 1.6089175939559937\n",
      "loss at i 4410: 1.4336333274841309\n",
      "loss at i 4420: 1.4535855054855347\n",
      "loss at i 4430: 1.509562611579895\n",
      "loss at i 4440: 1.3791977167129517\n",
      "loss at i 4450: 1.324104905128479\n",
      "loss at i 4460: 1.4247889518737793\n",
      "loss at i 4470: 1.473893642425537\n",
      "loss at i 4480: 1.3751691579818726\n",
      "loss at i 4490: 1.9220842123031616\n",
      "loss at i 4500: 1.8580093383789062\n",
      "loss at i 4510: 2.0588934421539307\n",
      "loss at i 4520: 1.7016245126724243\n",
      "loss at i 4530: 1.5636366605758667\n",
      "loss at i 4540: 1.7592060565948486\n",
      "loss at i 4550: 2.3068454265594482\n",
      "loss at i 4560: 1.8914109468460083\n",
      "loss at i 4570: 1.8521909713745117\n",
      "loss at i 4580: 1.6358909606933594\n",
      "loss at i 4590: 1.9447119235992432\n",
      "loss at i 4600: 2.031113624572754\n",
      "loss at i 4610: 1.6614097356796265\n",
      "loss at i 4620: 1.5724748373031616\n",
      "loss at i 4630: 2.09893536567688\n",
      "loss at i 4640: 2.2155301570892334\n",
      "loss at i 4650: 1.785059928894043\n",
      "loss at i 4660: 1.7411034107208252\n",
      "loss at i 4670: 1.2502870559692383\n",
      "loss at i 4680: 1.9203866720199585\n",
      "loss at i 4690: 1.9612475633621216\n",
      "loss at i 4700: 1.8758103847503662\n",
      "loss at i 4710: 1.4794952869415283\n",
      "loss at i 4720: 1.667039155960083\n",
      "loss at i 4730: 1.5365203619003296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at i 4740: 1.6993424892425537\n",
      "loss at i 4750: 1.9289298057556152\n",
      "loss at i 4760: 2.3035643100738525\n",
      "loss at i 4770: 1.6932036876678467\n",
      "loss at i 4780: 1.8010753393173218\n",
      "loss at i 4790: 1.9341217279434204\n",
      "loss at i 4800: 1.6575204133987427\n",
      "loss at i 4810: 1.4792699813842773\n",
      "loss at i 4820: 1.3333314657211304\n",
      "loss at i 4830: 1.4460935592651367\n",
      "loss at i 4840: 1.6035950183868408\n",
      "loss at i 4850: 1.5122863054275513\n",
      "loss at i 4860: 2.0259172916412354\n",
      "loss at i 4870: 1.9948936700820923\n",
      "loss at i 4880: 1.3695167303085327\n",
      "loss at i 4890: 1.7331959009170532\n",
      "loss at i 4900: 1.9805819988250732\n",
      "loss at i 4910: 1.7371567487716675\n",
      "loss at i 4920: 1.1094332933425903\n",
      "loss at i 4930: 1.8216477632522583\n",
      "loss at i 4940: 1.6491690874099731\n",
      "loss at i 4950: 1.3422423601150513\n",
      "loss at i 4960: 1.5432418584823608\n",
      "loss at i 4970: 2.018401861190796\n",
      "loss at i 4980: 1.8296537399291992\n",
      "loss at i 4990: 1.6752114295959473\n",
      "loss at i 5000: 1.967334270477295\n",
      "loss at i 5010: 1.6441985368728638\n",
      "loss at i 5020: 1.4928383827209473\n",
      "loss at i 5030: 1.6759337186813354\n",
      "loss at i 5040: 1.6622556447982788\n",
      "loss at i 5050: 1.28829026222229\n",
      "loss at i 5060: 1.3122529983520508\n",
      "loss at i 5070: 1.4527151584625244\n",
      "loss at i 5080: 1.7666150331497192\n",
      "loss at i 5090: 1.7400085926055908\n",
      "loss at i 5100: 1.6832369565963745\n",
      "loss at i 5110: 2.088672399520874\n",
      "loss at i 5120: 1.6696383953094482\n",
      "loss at i 5130: 1.4053186178207397\n",
      "loss at i 5140: 1.8480972051620483\n",
      "loss at i 5150: 1.71282160282135\n",
      "loss at i 5160: 1.561029314994812\n",
      "loss at i 5170: 1.7468973398208618\n",
      "loss at i 5180: 1.7825602293014526\n",
      "loss at i 5190: 1.9101769924163818\n",
      "loss at i 5200: 1.5449846982955933\n",
      "loss at i 5210: 1.7069854736328125\n",
      "loss at i 5220: 2.1265299320220947\n",
      "loss at i 5230: 1.7842673063278198\n",
      "loss at i 5240: 1.6350091695785522\n",
      "loss at i 5250: 1.3597956895828247\n",
      "loss at i 5260: 1.6565406322479248\n",
      "loss at i 5270: 1.3802039623260498\n",
      "loss at i 5280: 1.461812973022461\n",
      "loss at i 5290: 1.4959676265716553\n",
      "loss at i 5300: 1.4546046257019043\n",
      "loss at i 5310: 1.6506294012069702\n",
      "loss at i 5320: 1.8796417713165283\n",
      "loss at i 5330: 1.7386456727981567\n",
      "loss at i 5340: 1.88580322265625\n",
      "loss at i 5350: 2.1849660873413086\n",
      "loss at i 5360: 1.5137262344360352\n",
      "loss at i 5370: 1.6442009210586548\n",
      "loss at i 5380: 1.8414226770401\n",
      "loss at i 5390: 1.7343765497207642\n",
      "loss at i 5400: 1.7788666486740112\n",
      "loss at i 5410: 1.615218162536621\n",
      "loss at i 5420: 1.8051085472106934\n",
      "loss at i 5430: 1.7241789102554321\n",
      "loss at i 5440: 1.4290670156478882\n",
      "loss at i 5450: 1.849745273590088\n",
      "loss at i 5460: 1.5060882568359375\n",
      "loss at i 5470: 1.1634633541107178\n",
      "loss at i 5480: 1.4848921298980713\n",
      "loss at i 5490: 2.0378522872924805\n",
      "loss at i 5500: 1.9114646911621094\n",
      "loss at i 5510: 1.5128693580627441\n",
      "loss at i 5520: 1.574770212173462\n",
      "loss at i 5530: 1.7731492519378662\n",
      "loss at i 5540: 1.3110394477844238\n",
      "loss at i 5550: 1.5568934679031372\n",
      "loss at i 5560: 1.7143205404281616\n",
      "loss at i 5570: 1.7712112665176392\n",
      "loss at i 5580: 2.253528118133545\n",
      "loss at i 5590: 1.479139804840088\n",
      "loss at i 5600: 2.239544630050659\n",
      "loss at i 5610: 1.5961085557937622\n",
      "loss at i 5620: 1.6131720542907715\n",
      "loss at i 5630: 1.5989820957183838\n",
      "loss at i 5640: 1.4965424537658691\n",
      "loss at i 5650: 1.975638508796692\n",
      "loss at i 5660: 1.7714916467666626\n",
      "loss at i 5670: 1.459922432899475\n",
      "loss at i 5680: 1.6392589807510376\n",
      "loss at i 5690: 2.049445867538452\n",
      "loss at i 5700: 1.4893851280212402\n",
      "loss at i 5710: 2.0484390258789062\n",
      "loss at i 5720: 1.5581893920898438\n",
      "loss at i 5730: 2.000206708908081\n",
      "loss at i 5740: 1.8504172563552856\n",
      "loss at i 5750: 1.378771185874939\n",
      "loss at i 5760: 2.3065359592437744\n",
      "loss at i 5770: 1.8215882778167725\n",
      "loss at i 5780: 1.6171385049819946\n",
      "loss at i 5790: 1.5415905714035034\n",
      "loss at i 5800: 1.7570655345916748\n",
      "loss at i 5810: 1.9657416343688965\n",
      "loss at i 5820: 1.6691361665725708\n",
      "loss at i 5830: 1.8289750814437866\n",
      "loss at i 5840: 1.5942614078521729\n",
      "loss at i 5850: 1.6962616443634033\n",
      "loss at i 5860: 1.9493974447250366\n",
      "loss at i 5870: 1.7336235046386719\n",
      "loss at i 5880: 1.3652620315551758\n",
      "loss at i 5890: 1.9782130718231201\n",
      "loss at i 5900: 1.7998346090316772\n",
      "loss at i 5910: 2.1153132915496826\n",
      "loss at i 5920: 1.9752893447875977\n",
      "loss at i 5930: 1.660336971282959\n",
      "loss at i 5940: 2.4503588676452637\n",
      "loss at i 5950: 1.7257965803146362\n",
      "loss at i 5960: 1.8748767375946045\n",
      "loss at i 5970: 1.4311926364898682\n",
      "loss at i 5980: 1.824479341506958\n",
      "loss at i 5990: 1.7652724981307983\n",
      "loss at i 6000: 1.4027830362319946\n",
      "loss at i 6010: 1.8359917402267456\n",
      "loss at i 6020: 2.1307716369628906\n",
      "loss at i 6030: 1.8404337167739868\n",
      "loss at i 6040: 1.5955381393432617\n",
      "loss at i 6050: 1.5845848321914673\n",
      "loss at i 6060: 2.375074863433838\n",
      "loss at i 6070: 1.8069689273834229\n",
      "loss at i 6080: 1.6362179517745972\n",
      "loss at i 6090: 1.7490696907043457\n",
      "loss at i 6100: 1.2215821743011475\n",
      "loss at i 6110: 1.5647454261779785\n",
      "loss at i 6120: 1.8449848890304565\n",
      "loss at i 6130: 2.0227081775665283\n",
      "loss at i 6140: 1.6865959167480469\n",
      "loss at i 6150: 1.9917722940444946\n",
      "loss at i 6160: 1.5057964324951172\n",
      "loss at i 6170: 1.594028115272522\n",
      "loss at i 6180: 1.3145873546600342\n",
      "loss at i 6190: 1.6125173568725586\n",
      "loss at i 6200: 1.4516297578811646\n",
      "loss at i 6210: 1.3824495077133179\n",
      "loss at i 6220: 1.815112590789795\n",
      "loss at i 6230: 1.6116660833358765\n",
      "loss at i 6240: 1.97505784034729\n",
      "loss at i 6250: 1.557633638381958\n",
      "loss at i 6260: 1.5438995361328125\n",
      "loss at i 6270: 1.5569946765899658\n",
      "loss at i 6280: 1.3798154592514038\n",
      "loss at i 6290: 1.631189227104187\n",
      "loss at i 6300: 2.3628499507904053\n",
      "loss at i 6310: 1.5380163192749023\n",
      "loss at i 6320: 1.272770881652832\n",
      "loss at i 6330: 1.3023256063461304\n",
      "loss at i 6340: 1.6065385341644287\n",
      "loss at i 6350: 1.7397902011871338\n",
      "loss at i 6360: 1.5386465787887573\n",
      "loss at i 6370: 2.069685220718384\n",
      "loss at i 6380: 1.722701907157898\n",
      "loss at i 6390: 1.6676292419433594\n",
      "loss at i 6400: 1.7284746170043945\n",
      "loss at i 6410: 2.1122989654541016\n",
      "loss at i 6420: 1.7527657747268677\n",
      "loss at i 6430: 1.4407390356063843\n",
      "loss at i 6440: 2.3711812496185303\n",
      "loss at i 6450: 1.70435631275177\n",
      "loss at i 6460: 1.65435791015625\n",
      "loss at i 6470: 1.8765565156936646\n",
      "loss at i 6480: 2.0357067584991455\n",
      "loss at i 6490: 1.8087037801742554\n",
      "loss at i 6500: 1.3986608982086182\n",
      "loss at i 6510: 1.5013437271118164\n",
      "loss at i 6520: 1.5997830629348755\n",
      "loss at i 6530: 1.644271731376648\n",
      "loss at i 6540: 1.9055308103561401\n",
      "loss at i 6550: 1.4895142316818237\n",
      "loss at i 6560: 1.2832874059677124\n",
      "loss at i 6570: 1.2694238424301147\n",
      "loss at i 6580: 1.9435737133026123\n",
      "loss at i 6590: 1.4609558582305908\n",
      "loss at i 6600: 1.9518297910690308\n",
      "loss at i 6610: 1.803125262260437\n",
      "loss at i 6620: 1.6063416004180908\n",
      "loss at i 6630: 1.7475155591964722\n",
      "loss at i 6640: 2.176793098449707\n",
      "loss at i 6650: 1.2151480913162231\n",
      "loss at i 6660: 1.486915946006775\n",
      "loss at i 6670: 1.43728506565094\n",
      "loss at i 6680: 1.75360906124115\n",
      "loss at i 6690: 1.9448882341384888\n",
      "loss at i 6700: 2.2168524265289307\n",
      "loss at i 6710: 1.6121035814285278\n",
      "loss at i 6720: 1.5035609006881714\n",
      "loss at i 6730: 1.3571466207504272\n",
      "loss at i 6740: 1.6336300373077393\n",
      "loss at i 6750: 1.8200284242630005\n",
      "loss at i 6760: 1.2880315780639648\n",
      "loss at i 6770: 2.002535581588745\n",
      "loss at i 6780: 1.7698349952697754\n",
      "loss at i 6790: 1.9566127061843872\n",
      "loss at i 6800: 1.7919749021530151\n",
      "loss at i 6810: 2.0137510299682617\n",
      "loss at i 6820: 1.5668625831604004\n",
      "loss at i 6830: 1.3038896322250366\n",
      "loss at i 6840: 1.6715404987335205\n",
      "loss at i 6850: 1.6906949281692505\n",
      "loss at i 6860: 1.4140489101409912\n",
      "loss at i 6870: 1.9734668731689453\n",
      "loss at i 6880: 1.666695475578308\n",
      "loss at i 6890: 1.852226972579956\n",
      "loss at i 6900: 1.5364571809768677\n",
      "loss at i 6910: 1.7939702272415161\n",
      "loss at i 6920: 1.3763171434402466\n",
      "loss at i 6930: 1.46656334400177\n",
      "loss at i 6940: 1.8191407918930054\n",
      "loss at i 6950: 1.766573429107666\n",
      "loss at i 6960: 1.399167537689209\n",
      "loss at i 6970: 1.7536836862564087\n",
      "loss at i 6980: 1.805558443069458\n",
      "loss at i 6990: 1.58531653881073\n",
      "loss at i 7000: 2.0313503742218018\n",
      "loss at i 7010: 1.6032075881958008\n",
      "loss at i 7020: 1.6534639596939087\n",
      "loss at i 7030: 1.2346609830856323\n",
      "loss at i 7040: 1.758287787437439\n",
      "loss at i 7050: 1.498507022857666\n",
      "loss at i 7060: 1.811464786529541\n",
      "loss at i 7070: 1.6777594089508057\n",
      "loss at i 7080: 2.012388229370117\n",
      "loss at i 7090: 1.707694411277771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at i 7100: 1.5216366052627563\n",
      "loss at i 7110: 2.2214460372924805\n",
      "loss at i 7120: 1.8268442153930664\n",
      "loss at i 7130: 2.1025795936584473\n",
      "loss at i 7140: 1.8093243837356567\n",
      "loss at i 7150: 1.7985748052597046\n",
      "loss at i 7160: 2.3780689239501953\n",
      "loss at i 7170: 2.591331720352173\n",
      "loss at i 7180: 1.9981609582901\n",
      "loss at i 7190: 1.816736102104187\n",
      "loss at i 7200: 1.3454113006591797\n",
      "loss at i 7210: 2.162644386291504\n",
      "loss at i 7220: 1.4984389543533325\n",
      "loss at i 7230: 1.5378327369689941\n",
      "loss at i 7240: 1.6162934303283691\n",
      "loss at i 7250: 1.6243751049041748\n",
      "loss at i 7260: 1.521057367324829\n",
      "loss at i 7270: 1.9787521362304688\n",
      "loss at i 7280: 1.9871846437454224\n",
      "loss at i 7290: 2.6016650199890137\n",
      "loss at i 7300: 1.6835061311721802\n",
      "loss at i 7310: 1.3180091381072998\n",
      "loss at i 7320: 1.8899675607681274\n",
      "loss at i 7330: 1.6435832977294922\n",
      "loss at i 7340: 2.1245462894439697\n",
      "loss at i 7350: 1.9004918336868286\n",
      "loss at i 7360: 2.2346608638763428\n",
      "loss at i 7370: 1.4983867406845093\n",
      "loss at i 7380: 1.3905813694000244\n",
      "loss at i 7390: 1.88002347946167\n",
      "loss at i 7400: 1.6667077541351318\n",
      "loss at i 7410: 1.4027366638183594\n",
      "loss at i 7420: 1.4800679683685303\n",
      "loss at i 7430: 1.7079578638076782\n",
      "loss at i 7440: 1.4577486515045166\n",
      "loss at i 7450: 1.5306307077407837\n",
      "loss at i 7460: 1.946677803993225\n",
      "loss at i 7470: 2.1658358573913574\n",
      "loss at i 7480: 2.132671594619751\n",
      "loss at i 7490: 1.848515272140503\n",
      "loss at i 7500: 1.390039086341858\n",
      "loss at i 7510: 1.5124375820159912\n",
      "loss at i 7520: 1.9487236738204956\n",
      "loss at i 7530: 1.5772615671157837\n",
      "loss at i 7540: 2.0535519123077393\n",
      "loss at i 7550: 1.897924542427063\n",
      "loss at i 7560: 1.928830623626709\n",
      "loss at i 7570: 1.8508615493774414\n",
      "loss at i 7580: 2.2352614402770996\n",
      "loss at i 7590: 2.1757895946502686\n",
      "loss at i 7600: 1.6389554738998413\n",
      "loss at i 7610: 1.6865928173065186\n",
      "loss at i 7620: 1.8205242156982422\n",
      "loss at i 7630: 1.9574686288833618\n",
      "loss at i 7640: 1.7627112865447998\n",
      "loss at i 7650: 1.7586201429367065\n",
      "loss at i 7660: 1.306432843208313\n",
      "loss at i 7670: 1.939504623413086\n",
      "loss at i 7680: 2.0140769481658936\n",
      "loss at i 7690: 1.6896435022354126\n",
      "loss at i 7700: 1.7660385370254517\n",
      "loss at i 7710: 1.733097791671753\n",
      "loss at i 7720: 2.0509421825408936\n",
      "loss at i 7730: 1.614668846130371\n",
      "loss at i 7740: 1.7281469106674194\n",
      "loss at i 7750: 1.2943801879882812\n",
      "loss at i 7760: 2.0252764225006104\n",
      "loss at i 7770: 1.5352699756622314\n",
      "loss at i 7780: 2.0144131183624268\n",
      "loss at i 7790: 1.3992149829864502\n",
      "loss at i 7800: 1.371373176574707\n",
      "loss at i 7810: 1.9191555976867676\n",
      "loss at i 7820: 1.7050049304962158\n",
      "loss at i 7830: 2.1424570083618164\n",
      "loss at i 7840: 2.2675185203552246\n",
      "loss at i 7850: 1.8590152263641357\n",
      "loss at i 7860: 2.0984127521514893\n",
      "loss at i 7870: 1.758469820022583\n",
      "loss at i 7880: 1.411421298980713\n",
      "loss at i 7890: 1.8333461284637451\n",
      "loss at i 7900: 1.8019073009490967\n",
      "loss at i 7910: 1.8295042514801025\n",
      "loss at i 7920: 1.8965998888015747\n",
      "loss at i 7930: 1.6598378419876099\n",
      "loss at i 7940: 2.2395682334899902\n",
      "loss at i 7950: 1.5460489988327026\n",
      "loss at i 7960: 2.186551332473755\n",
      "loss at i 7970: 1.6371665000915527\n",
      "loss at i 7980: 1.4164910316467285\n",
      "loss at i 7990: 1.4768552780151367\n",
      "loss at i 8000: 1.5984035730361938\n",
      "loss at i 8010: 1.7349988222122192\n",
      "loss at i 8020: 1.9590117931365967\n",
      "loss at i 8030: 1.8716809749603271\n",
      "loss at i 8040: 2.2921576499938965\n",
      "loss at i 8050: 1.6330604553222656\n",
      "loss at i 8060: 1.2764838933944702\n",
      "loss at i 8070: 1.5842655897140503\n",
      "loss at i 8080: 1.5688683986663818\n",
      "loss at i 8090: 1.3511993885040283\n",
      "loss at i 8100: 1.5634793043136597\n",
      "loss at i 8110: 1.8022253513336182\n",
      "loss at i 8120: 1.6548902988433838\n",
      "loss at i 8130: 1.5429091453552246\n",
      "loss at i 8140: 1.334930658340454\n",
      "loss at i 8150: 1.780868411064148\n",
      "loss at i 8160: 1.6105551719665527\n",
      "loss at i 8170: 1.670397400856018\n",
      "loss at i 8180: 1.6060231924057007\n",
      "loss at i 8190: 1.6668987274169922\n",
      "loss at i 8200: 1.8766807317733765\n",
      "loss at i 8210: 1.5955854654312134\n",
      "loss at i 8220: 1.9827332496643066\n",
      "loss at i 8230: 1.136232852935791\n",
      "loss at i 8240: 1.7820314168930054\n",
      "loss at i 8250: 1.632169246673584\n",
      "loss at i 8260: 1.6033105850219727\n",
      "loss at i 8270: 1.8926385641098022\n",
      "loss at i 8280: 1.3852499723434448\n",
      "loss at i 8290: 1.9406934976577759\n",
      "loss at i 8300: 1.7066235542297363\n",
      "loss at i 8310: 1.8236560821533203\n",
      "loss at i 8320: 2.0379817485809326\n",
      "loss at i 8330: 1.3049222230911255\n",
      "loss at i 8340: 2.023705244064331\n",
      "loss at i 8350: 1.718062400817871\n",
      "loss at i 8360: 1.6469553709030151\n",
      "loss at i 8370: 2.0879859924316406\n",
      "loss at i 8380: 1.6101760864257812\n",
      "loss at i 8390: 1.9416909217834473\n",
      "loss at i 8400: 1.9293313026428223\n",
      "loss at i 8410: 1.5507841110229492\n",
      "loss at i 8420: 1.9209538698196411\n",
      "loss at i 8430: 1.7501931190490723\n",
      "loss at i 8440: 1.5358670949935913\n",
      "loss at i 8450: 2.0949466228485107\n",
      "loss at i 8460: 1.7622572183609009\n",
      "loss at i 8470: 1.96241295337677\n",
      "loss at i 8480: 1.97976553440094\n",
      "loss at i 8490: 2.511014699935913\n",
      "loss at i 8500: 1.1622397899627686\n",
      "loss at i 8510: 1.7561304569244385\n",
      "loss at i 8520: 2.00019907951355\n",
      "loss at i 8530: 2.2864277362823486\n",
      "loss at i 8540: 1.8363686800003052\n",
      "loss at i 8550: 1.658573031425476\n",
      "loss at i 8560: 2.058802366256714\n",
      "loss at i 8570: 1.6396955251693726\n",
      "loss at i 8580: 1.6009674072265625\n",
      "loss at i 8590: 1.7431832551956177\n",
      "loss at i 8600: 1.404174566268921\n",
      "loss at i 8610: 1.6223887205123901\n",
      "loss at i 8620: 1.4227328300476074\n",
      "loss at i 8630: 2.0047006607055664\n",
      "loss at i 8640: 2.121917963027954\n",
      "loss at i 8650: 1.5642917156219482\n",
      "loss at i 8660: 1.7617048025131226\n",
      "loss at i 8670: 1.6973166465759277\n",
      "loss at i 8680: 1.973170280456543\n",
      "loss at i 8690: 1.4057096242904663\n",
      "loss at i 8700: 1.7739280462265015\n",
      "loss at i 8710: 1.98495614528656\n",
      "loss at i 8720: 1.7943931818008423\n",
      "loss at i 8730: 1.2540459632873535\n",
      "loss at i 8740: 1.7500793933868408\n",
      "loss at i 8750: 1.4630348682403564\n",
      "loss at i 8760: 1.5684471130371094\n",
      "loss at i 8770: 1.5380913019180298\n",
      "loss at i 8780: 1.8397462368011475\n",
      "loss at i 8790: 1.6006553173065186\n",
      "loss at i 8800: 1.6740403175354004\n",
      "loss at i 8810: 2.0037012100219727\n",
      "loss at i 8820: 1.220373511314392\n",
      "loss at i 8830: 2.057338237762451\n",
      "loss at i 8840: 2.1437456607818604\n",
      "loss at i 8850: 1.5944753885269165\n",
      "loss at i 8860: 1.602929949760437\n",
      "loss at i 8870: 1.8903979063034058\n",
      "loss at i 8880: 1.7217196226119995\n",
      "loss at i 8890: 1.6256455183029175\n",
      "loss at i 8900: 1.9837006330490112\n",
      "loss at i 8910: 1.8651199340820312\n",
      "loss at i 8920: 1.8111432790756226\n",
      "loss at i 8930: 2.102140188217163\n",
      "loss at i 8940: 2.01364803314209\n",
      "loss at i 8950: 1.5616689920425415\n",
      "loss at i 8960: 1.6006571054458618\n",
      "loss at i 8970: 1.712276577949524\n",
      "loss at i 8980: 1.3561311960220337\n",
      "loss at i 8990: 1.6561979055404663\n",
      "loss at i 9000: 1.7199950218200684\n",
      "loss at i 9010: 1.8988983631134033\n",
      "loss at i 9020: 1.5078611373901367\n",
      "loss at i 9030: 2.015214681625366\n",
      "loss at i 9040: 2.351233959197998\n",
      "loss at i 9050: 1.9948740005493164\n",
      "loss at i 9060: 1.8153126239776611\n",
      "loss at i 9070: 1.5285210609436035\n",
      "loss at i 9080: 1.6958519220352173\n",
      "loss at i 9090: 2.1105382442474365\n",
      "loss at i 9100: 1.6014301776885986\n",
      "loss at i 9110: 2.0410842895507812\n",
      "loss at i 9120: 1.4218785762786865\n",
      "loss at i 9130: 2.225619077682495\n",
      "loss at i 9140: 1.6128010749816895\n",
      "loss at i 9150: 1.7024478912353516\n",
      "loss at i 9160: 1.8589903116226196\n",
      "loss at i 9170: 1.458835244178772\n",
      "loss at i 9180: 1.3565330505371094\n",
      "loss at i 9190: 1.655969500541687\n",
      "loss at i 9200: 1.4202826023101807\n",
      "loss at i 9210: 1.4791432619094849\n",
      "loss at i 9220: 1.7393357753753662\n",
      "loss at i 9230: 1.468567132949829\n",
      "loss at i 9240: 1.3068337440490723\n",
      "loss at i 9250: 1.2093956470489502\n",
      "loss at i 9260: 1.4012774229049683\n",
      "loss at i 9270: 1.79025399684906\n",
      "loss at i 9280: 2.2226059436798096\n",
      "loss at i 9290: 1.5282645225524902\n",
      "loss at i 9300: 2.0129659175872803\n",
      "loss at i 9310: 1.9484044313430786\n",
      "loss at i 9320: 1.8216334581375122\n",
      "loss at i 9330: 1.6251696348190308\n",
      "loss at i 9340: 1.6976096630096436\n",
      "loss at i 9350: 1.4327408075332642\n",
      "loss at i 9360: 1.6426761150360107\n",
      "loss at i 9370: 1.843307375907898\n",
      "loss at i 9380: 1.647670030593872\n",
      "loss at i 9390: 1.6724505424499512\n",
      "loss at i 9400: 1.9278123378753662\n",
      "loss at i 9410: 2.0411548614501953\n",
      "loss at i 9420: 2.138674020767212\n",
      "loss at i 9430: 1.6917800903320312\n",
      "loss at i 9440: 1.6251176595687866\n",
      "loss at i 9450: 1.9114322662353516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at i 9460: 1.374603509902954\n",
      "loss at i 9470: 1.8527960777282715\n",
      "loss at i 9480: 2.0412962436676025\n",
      "loss at i 9490: 1.14285409450531\n",
      "loss at i 9500: 2.0627753734588623\n",
      "loss at i 9510: 1.8791292905807495\n",
      "loss at i 9520: 2.0615267753601074\n",
      "loss at i 9530: 1.5503530502319336\n",
      "loss at i 9540: 1.341149091720581\n",
      "loss at i 9550: 1.9804472923278809\n",
      "loss at i 9560: 1.395098328590393\n",
      "loss at i 9570: 1.7393008470535278\n",
      "loss at i 9580: 1.9806724786758423\n",
      "loss at i 9590: 1.4780060052871704\n",
      "loss at i 9600: 1.8604604005813599\n",
      "loss at i 9610: 2.005167007446289\n",
      "loss at i 9620: 1.9821746349334717\n",
      "loss at i 9630: 1.728538155555725\n",
      "loss at i 9640: 1.513158917427063\n",
      "loss at i 9650: 1.4065732955932617\n",
      "loss at i 9660: 1.9304578304290771\n",
      "loss at i 9670: 1.8239959478378296\n",
      "loss at i 9680: 1.4215291738510132\n",
      "loss at i 9690: 2.1249008178710938\n",
      "loss at i 9700: 1.9312522411346436\n",
      "loss at i 9710: 1.3415294885635376\n",
      "loss at i 9720: 1.3063488006591797\n",
      "loss at i 9730: 1.5919761657714844\n",
      "loss at i 9740: 1.5957603454589844\n",
      "loss at i 9750: 1.3759979009628296\n",
      "loss at i 9760: 2.039618968963623\n",
      "loss at i 9770: 1.5618689060211182\n",
      "loss at i 9780: 1.3890306949615479\n",
      "loss at i 9790: 1.6352756023406982\n",
      "loss at i 9800: 1.7991116046905518\n",
      "loss at i 9810: 1.6078530550003052\n",
      "loss at i 9820: 1.8546748161315918\n",
      "loss at i 9830: 1.5719738006591797\n",
      "loss at i 9840: 1.7824586629867554\n",
      "loss at i 9850: 1.6558308601379395\n",
      "loss at i 9860: 2.020433187484741\n",
      "loss at i 9870: 1.767938256263733\n",
      "loss at i 9880: 1.4468913078308105\n",
      "loss at i 9890: 1.7774173021316528\n",
      "loss at i 9900: 1.9201802015304565\n",
      "loss at i 9910: 1.4499917030334473\n",
      "loss at i 9920: 1.8301353454589844\n",
      "loss at i 9930: 1.5195269584655762\n",
      "loss at i 9940: 1.5606755018234253\n",
      "loss at i 9950: 2.567227602005005\n",
      "loss at i 9960: 1.5635846853256226\n",
      "loss at i 9970: 1.4908033609390259\n",
      "loss at i 9980: 1.9251428842544556\n",
      "loss at i 9990: 1.6446791887283325\n"
     ]
    }
   ],
   "source": [
    "#### Training Phase ###############\n",
    "EPOCHS     = 10 # Will need to figure out what this should be. dVC stuff?\n",
    "ITERATIONS = 10000\n",
    "\n",
    "# NOTE: Hacky right now, but just want to get data into the model.\n",
    "# TODO: actually turn the data into tf.Dataset objects? \n",
    "# TODO: or use some of the batch operations?\n",
    "def get_training_batch(X, f, y, batch_size):\n",
    "    ids = np.random.randint(0, len(X), batch_size) \n",
    "    return np.array(X)[ids], np.array(f)[ids], np.array(y)[ids]\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Testing for now, want to see if it actually updates on one batch.\n",
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.summary.FileWriter('../data', sess.graph)\n",
    "    init.run()\n",
    "    \n",
    "    # For debugging\n",
    "    X_batch, F_batch, y_batch = get_training_batch( X_full, f_full, y_full, BATCH_SIZE )\n",
    "    print(\"shape(F): \",  sess.run(F, feed_dict={X: X_batch, F: F_batch, y: y_batch}).shape)\n",
    "    print(\"shape(tiled_features): \",  sess.run(combined_outputs, feed_dict={X: X_batch, F: F_batch, y: y_batch}).shape)    \n",
    "  \n",
    "    for i in range(ITERATIONS):\n",
    "        X_batch, F_batch, y_batch = get_training_batch( X_full, f_full, y_full, BATCH_SIZE )   \n",
    "        _, l, summary = sess.run([training_op, loss, merged], feed_dict={X: X_batch, F: F_batch, y: y_batch})\n",
    "        summary_writer.add_summary(summary, i)\n",
    "        if i%10 == 0: print(\"loss at i {}: {}\".format(i, l))\n",
    "            \n",
    "    summary_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
